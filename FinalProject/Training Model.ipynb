{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:40: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/gmo/.local/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/gmo/.local/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:46: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmo/.local/lib/python3.5/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Frame 138 - Frames/sec 143.0 - Epsilon 1.0 - Mean Reward 0.4927536231884058\n",
      "Episode 100 - Frame 27052 - Frames/sec 135.0 - Epsilon 0.9933 - Mean Reward 0.6036848997745411\n",
      "Episode 200 - Frame 50063 - Frames/sec 137.0 - Epsilon 0.9876 - Mean Reward 0.5952738805014954\n",
      "Episode 300 - Frame 83916 - Frames/sec 139.0 - Epsilon 0.9792 - Mean Reward 0.5307022568916109\n",
      "Episode 400 - Frame 119775 - Frames/sec 88.0 - Epsilon 0.9705 - Mean Reward 0.48851863692766045\n",
      "Episode 500 - Frame 149659 - Frames/sec 67.0 - Epsilon 0.9633 - Mean Reward 0.5456574488190917\n",
      "Episode 600 - Frame 184265 - Frames/sec 65.0 - Epsilon 0.955 - Mean Reward 0.5164309098626186\n",
      "Episode 700 - Frame 215877 - Frames/sec 65.0 - Epsilon 0.9475 - Mean Reward 0.5329728993970733\n",
      "Episode 800 - Frame 246620 - Frames/sec 66.0 - Epsilon 0.9402 - Mean Reward 0.539137052216338\n",
      "Episode 900 - Frame 274017 - Frames/sec 65.0 - Epsilon 0.9338 - Mean Reward 0.5489943196366907\n",
      "Episode 1000 - Frame 302215 - Frames/sec 66.0 - Epsilon 0.9272 - Mean Reward 0.5607849970968678\n",
      "Episode 1100 - Frame 323141 - Frames/sec 66.0 - Epsilon 0.9224 - Mean Reward 0.6307624399371881\n",
      "Episode 1200 - Frame 348231 - Frames/sec 66.0 - Epsilon 0.9166 - Mean Reward 0.55788018469348\n",
      "Episode 1300 - Frame 375435 - Frames/sec 66.0 - Epsilon 0.9104 - Mean Reward 0.5433206256113868\n",
      "Episode 1400 - Frame 405752 - Frames/sec 67.0 - Epsilon 0.9035 - Mean Reward 0.5043905429698453\n",
      "Episode 1500 - Frame 433313 - Frames/sec 66.0 - Epsilon 0.8973 - Mean Reward 0.5371578714886683\n",
      "Episode 1600 - Frame 462262 - Frames/sec 66.0 - Epsilon 0.8909 - Mean Reward 0.5494443791286904\n",
      "Episode 1700 - Frame 487936 - Frames/sec 67.0 - Epsilon 0.8852 - Mean Reward 0.5832369067430803\n",
      "Episode 1800 - Frame 513109 - Frames/sec 67.0 - Epsilon 0.8796 - Mean Reward 0.5632365769467611\n",
      "Episode 1900 - Frame 539186 - Frames/sec 66.0 - Epsilon 0.8739 - Mean Reward 0.5497217659964703\n",
      "Episode 2000 - Frame 558808 - Frames/sec 67.0 - Epsilon 0.8696 - Mean Reward 0.635508595036725\n",
      "Episode 2100 - Frame 586698 - Frames/sec 67.0 - Epsilon 0.8636 - Mean Reward 0.5404431794117971\n",
      "Episode 2200 - Frame 617730 - Frames/sec 67.0 - Epsilon 0.8569 - Mean Reward 0.5148397958081753\n",
      "Episode 2300 - Frame 641643 - Frames/sec 67.0 - Epsilon 0.8518 - Mean Reward 0.5960785482209779\n",
      "Episode 2400 - Frame 663285 - Frames/sec 67.0 - Epsilon 0.8472 - Mean Reward 0.5872273897492112\n",
      "Episode 2500 - Frame 682316 - Frames/sec 67.0 - Epsilon 0.8432 - Mean Reward 0.6286215441883002\n",
      "Episode 2600 - Frame 709546 - Frames/sec 67.0 - Epsilon 0.8375 - Mean Reward 0.523291745719051\n",
      "Episode 2700 - Frame 733024 - Frames/sec 67.0 - Epsilon 0.8326 - Mean Reward 0.5788251927076672\n",
      "Episode 2800 - Frame 751774 - Frames/sec 67.0 - Epsilon 0.8287 - Mean Reward 0.6176912451548532\n",
      "Episode 2900 - Frame 775247 - Frames/sec 67.0 - Epsilon 0.8238 - Mean Reward 0.5697998272870073\n",
      "Episode 3000 - Frame 796840 - Frames/sec 67.0 - Epsilon 0.8194 - Mean Reward 0.582720636486041\n",
      "Episode 3100 - Frame 814939 - Frames/sec 67.0 - Epsilon 0.8157 - Mean Reward 0.6056495215326783\n",
      "Episode 3200 - Frame 840653 - Frames/sec 68.0 - Epsilon 0.8105 - Mean Reward 0.5579998092274356\n",
      "Episode 3300 - Frame 864220 - Frames/sec 67.0 - Epsilon 0.8057 - Mean Reward 0.550169424289125\n",
      "Episode 3400 - Frame 887133 - Frames/sec 67.0 - Epsilon 0.8011 - Mean Reward 0.573539470739185\n",
      "Episode 3500 - Frame 911630 - Frames/sec 67.0 - Epsilon 0.7962 - Mean Reward 0.5808530354051792\n",
      "Episode 3600 - Frame 939724 - Frames/sec 67.0 - Epsilon 0.7906 - Mean Reward 0.5322249436618719\n",
      "Episode 3700 - Frame 969108 - Frames/sec 67.0 - Epsilon 0.7848 - Mean Reward 0.5094062448009203\n",
      "Episode 3800 - Frame 994139 - Frames/sec 67.0 - Epsilon 0.7799 - Mean Reward 0.5907023537959537\n",
      "Episode 3900 - Frame 1019136 - Frames/sec 67.0 - Epsilon 0.7751 - Mean Reward 0.5472657402179746\n",
      "Episode 4000 - Frame 1045502 - Frames/sec 67.0 - Epsilon 0.77 - Mean Reward 0.5585068750628764\n",
      "Episode 4100 - Frame 1070151 - Frames/sec 67.0 - Epsilon 0.7653 - Mean Reward 0.5579862158927203\n",
      "Episode 4200 - Frame 1093293 - Frames/sec 66.0 - Epsilon 0.7608 - Mean Reward 0.5677353756775154\n",
      "Episode 4300 - Frame 1119321 - Frames/sec 67.0 - Epsilon 0.7559 - Mean Reward 0.5724970725405707\n",
      "Episode 4400 - Frame 1140183 - Frames/sec 67.0 - Epsilon 0.752 - Mean Reward 0.6350869181923698\n",
      "Episode 4500 - Frame 1162646 - Frames/sec 68.0 - Epsilon 0.7478 - Mean Reward 0.5715368731139336\n",
      "Episode 4600 - Frame 1190212 - Frames/sec 67.0 - Epsilon 0.7426 - Mean Reward 0.53418512268162\n",
      "Episode 4700 - Frame 1220248 - Frames/sec 66.0 - Epsilon 0.7371 - Mean Reward 0.5451363758928318\n",
      "Episode 4800 - Frame 1244139 - Frames/sec 67.0 - Epsilon 0.7327 - Mean Reward 0.5688115297210672\n",
      "Episode 4900 - Frame 1265988 - Frames/sec 67.0 - Epsilon 0.7287 - Mean Reward 0.6127410991396968\n",
      "Episode 5000 - Frame 1287823 - Frames/sec 67.0 - Epsilon 0.7247 - Mean Reward 0.5841374136657155\n",
      "Episode 5100 - Frame 1313189 - Frames/sec 68.0 - Epsilon 0.7201 - Mean Reward 0.5686309602647026\n",
      "Episode 5200 - Frame 1330510 - Frames/sec 68.0 - Epsilon 0.717 - Mean Reward 0.6237874180016166\n",
      "Episode 5300 - Frame 1353711 - Frames/sec 67.0 - Epsilon 0.7129 - Mean Reward 0.5543212499591301\n",
      "Episode 5400 - Frame 1376566 - Frames/sec 67.0 - Epsilon 0.7088 - Mean Reward 0.6027889022453458\n",
      "Episode 5500 - Frame 1392998 - Frames/sec 67.0 - Epsilon 0.7059 - Mean Reward 0.6946704803030288\n",
      "Episode 5600 - Frame 1414041 - Frames/sec 66.0 - Epsilon 0.7022 - Mean Reward 0.6095530835684673\n",
      "Episode 5700 - Frame 1434072 - Frames/sec 67.0 - Epsilon 0.6987 - Mean Reward 0.5851325605001092\n",
      "Episode 5800 - Frame 1460459 - Frames/sec 66.0 - Epsilon 0.6941 - Mean Reward 0.5515163473174721\n",
      "Episode 5900 - Frame 1482888 - Frames/sec 66.0 - Epsilon 0.6902 - Mean Reward 0.6025444396449912\n",
      "Episode 6000 - Frame 1506702 - Frames/sec 67.0 - Epsilon 0.6861 - Mean Reward 0.5713394027008194\n",
      "Episode 6100 - Frame 1526073 - Frames/sec 67.0 - Epsilon 0.6828 - Mean Reward 0.6431576214733714\n",
      "Episode 6200 - Frame 1548053 - Frames/sec 66.0 - Epsilon 0.6791 - Mean Reward 0.6154331987354772\n",
      "Episode 6300 - Frame 1572392 - Frames/sec 66.0 - Epsilon 0.675 - Mean Reward 0.5892463053007277\n",
      "Episode 6400 - Frame 1595075 - Frames/sec 66.0 - Epsilon 0.6711 - Mean Reward 0.61124737825742\n",
      "Episode 6500 - Frame 1615315 - Frames/sec 67.0 - Epsilon 0.6678 - Mean Reward 0.5926974351000407\n",
      "Episode 6600 - Frame 1635822 - Frames/sec 67.0 - Epsilon 0.6643 - Mean Reward 0.6193674962395775\n",
      "Episode 6700 - Frame 1654641 - Frames/sec 67.0 - Epsilon 0.6612 - Mean Reward 0.6183928928346482\n",
      "Episode 6800 - Frame 1678310 - Frames/sec 67.0 - Epsilon 0.6573 - Mean Reward 0.5681188445647701\n",
      "Episode 6900 - Frame 1698315 - Frames/sec 67.0 - Epsilon 0.654 - Mean Reward 0.6065677296592235\n",
      "Episode 7000 - Frame 1718659 - Frames/sec 66.0 - Epsilon 0.6507 - Mean Reward 0.6312835340148295\n",
      "Episode 7100 - Frame 1735738 - Frames/sec 67.0 - Epsilon 0.648 - Mean Reward 0.6703304459901038\n",
      "Episode 7200 - Frame 1753773 - Frames/sec 66.0 - Epsilon 0.645 - Mean Reward 0.6881357842747883\n",
      "Episode 7300 - Frame 1775170 - Frames/sec 67.0 - Epsilon 0.6416 - Mean Reward 0.6143554782003506\n",
      "Episode 7400 - Frame 1793516 - Frames/sec 66.0 - Epsilon 0.6387 - Mean Reward 0.6532453783198491\n",
      "Episode 7500 - Frame 1814220 - Frames/sec 66.0 - Epsilon 0.6354 - Mean Reward 0.6172532475128769\n",
      "Episode 7600 - Frame 1834503 - Frames/sec 67.0 - Epsilon 0.6322 - Mean Reward 0.6599685936528403\n",
      "Episode 7700 - Frame 1851835 - Frames/sec 66.0 - Epsilon 0.6294 - Mean Reward 0.6880161935393582\n",
      "Episode 7800 - Frame 1872637 - Frames/sec 67.0 - Epsilon 0.6262 - Mean Reward 0.6244098583842047\n",
      "Episode 7900 - Frame 1892768 - Frames/sec 66.0 - Epsilon 0.623 - Mean Reward 0.6318968129701655\n",
      "Episode 8000 - Frame 1913106 - Frames/sec 67.0 - Epsilon 0.6199 - Mean Reward 0.6391742175613476\n",
      "Episode 8100 - Frame 1932267 - Frames/sec 67.0 - Epsilon 0.6169 - Mean Reward 0.6143254375586227\n",
      "Episode 8200 - Frame 1950978 - Frames/sec 67.0 - Epsilon 0.614 - Mean Reward 0.6590899468849635\n",
      "Episode 8300 - Frame 1980494 - Frames/sec 67.0 - Epsilon 0.6095 - Mean Reward 0.5658022704342018\n",
      "Episode 8400 - Frame 2003378 - Frames/sec 67.0 - Epsilon 0.606 - Mean Reward 0.6159897088786825\n",
      "Episode 8500 - Frame 2025714 - Frames/sec 66.0 - Epsilon 0.6026 - Mean Reward 0.6153154265177442\n",
      "Episode 8600 - Frame 2045027 - Frames/sec 67.0 - Epsilon 0.5997 - Mean Reward 0.6509337882017218\n",
      "Episode 8700 - Frame 2064901 - Frames/sec 67.0 - Epsilon 0.5968 - Mean Reward 0.6455910341248601\n",
      "Episode 8800 - Frame 2089336 - Frames/sec 67.0 - Epsilon 0.5931 - Mean Reward 0.6125679489292534\n",
      "Episode 8900 - Frame 2110666 - Frames/sec 67.0 - Epsilon 0.59 - Mean Reward 0.6497589316693787\n",
      "Episode 9000 - Frame 2131367 - Frames/sec 66.0 - Epsilon 0.5869 - Mean Reward 0.6243303320692334\n",
      "Episode 9100 - Frame 2149273 - Frames/sec 67.0 - Epsilon 0.5843 - Mean Reward 0.674329036268892\n",
      "Episode 9200 - Frame 2168969 - Frames/sec 67.0 - Epsilon 0.5814 - Mean Reward 0.6514366017923138\n",
      "Episode 9300 - Frame 2187467 - Frames/sec 66.0 - Epsilon 0.5788 - Mean Reward 0.6889948689724418\n",
      "Episode 9400 - Frame 2205640 - Frames/sec 67.0 - Epsilon 0.5761 - Mean Reward 0.6825960002728282\n",
      "Episode 9500 - Frame 2227929 - Frames/sec 67.0 - Epsilon 0.5729 - Mean Reward 0.6518066940537116\n",
      "Episode 9600 - Frame 2249053 - Frames/sec 66.0 - Epsilon 0.5699 - Mean Reward 0.6180702005436742\n",
      "Episode 9700 - Frame 2268792 - Frames/sec 67.0 - Epsilon 0.5671 - Mean Reward 0.6316109398692331\n",
      "Episode 9800 - Frame 2292046 - Frames/sec 67.0 - Epsilon 0.5638 - Mean Reward 0.6072516962938083\n",
      "Episode 9900 - Frame 2311020 - Frames/sec 67.0 - Epsilon 0.5612 - Mean Reward 0.6341206360764552\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from agent import DQNAgent\n",
    "from wrappers import wrapper\n",
    "\n",
    "\n",
    "# Build env (first level, right only)\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "env = wrapper(env)\n",
    "\n",
    "# Parameters\n",
    "states = (84, 84, 4)\n",
    "actions = env.action_space.n\n",
    "\n",
    "# Agent\n",
    "agent = DQNAgent(states=states, actions=actions, max_memory=100000, double_q=True)\n",
    "\n",
    "# Episodes\n",
    "episodes = 10000\n",
    "rewards = []\n",
    "\n",
    "# Timing\n",
    "start = time.time()\n",
    "step = 0\n",
    "\n",
    "# Main loop\n",
    "for e in range(episodes):\n",
    "\n",
    "    # Reset env\n",
    "    state = env.reset()\n",
    "\n",
    "    # Reward\n",
    "    total_reward = 0\n",
    "    iter = 0\n",
    "\n",
    "    # Play\n",
    "    while True:\n",
    "\n",
    "        # Show env\n",
    "        # env.render()\n",
    "\n",
    "        # Run agent\n",
    "        action = agent.run(state=state)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, info = env.step(action=action)\n",
    "\n",
    "        # Remember\n",
    "        agent.add(experience=(state, next_state, action, reward, done))\n",
    "\n",
    "        # Replay\n",
    "        agent.learn()\n",
    "\n",
    "        # Total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Increment\n",
    "        iter += 1\n",
    "\n",
    "        # If done break loop\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    # Rewards\n",
    "    rewards.append(total_reward / iter)\n",
    "\n",
    "    # Print\n",
    "    if e % 100 == 0:\n",
    "        print('Episode {e} - '\n",
    "              'Frame {f} - '\n",
    "              'Frames/sec {fs} - '\n",
    "              'Epsilon {eps} - '\n",
    "              'Mean Reward {r}'.format(e=e,\n",
    "                                       f=agent.step,\n",
    "                                       fs=np.round((agent.step - step) / (time.time() - start)),\n",
    "                                       eps=np.round(agent.eps, 4),\n",
    "                                       r=np.mean(rewards[-100:])))\n",
    "        start = time.time()\n",
    "        step = agent.step\n",
    "\n",
    "# Save rewards\n",
    "np.save('rewards.npy', rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
